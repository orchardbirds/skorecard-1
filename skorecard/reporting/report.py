import pandas as pd
import numpy as np
from typing import Union, Optional

from skorecard.bucket_mapping import BucketMapping
from skorecard.bucketers import UserInputBucketer


def bucket_table(
    x_original: pd.Series,
    x_bucketed: pd.Series,
    y: Union[pd.Series, np.array],
    bucket_mapping: Optional[BucketMapping] = None,
    epsilon: float = 0.00001,
) -> pd.DataFrame:
    """Create a table with results of bucketing.

    Example:

    ```python
    from skorecard import datasets
    from skorecard.bucketers import DecisionTreeBucketer
    from skorecard.reporting import bucket_table

    X, y = datasets.load_uci_credit_card(return_X_y=True)
    db = DecisionTreeBucketer(max_n_bins=7, min_bin_size=0.05)
    X_trans = db.fit_transform(X, y)
    x_original = X['BILL_AMT1']
    x_bucketed = X_trans['BILL_AMT1']

    bucket_table(x_original, x_bucketed, y)

    # or with a bucket_mapping for nice bucket range printing
    bucket_table(x_original, x_bucketed, y,
        bucket_mapping = db.features_bucket_mapping_.get('BILL_AMT1')
    )
    ```
    """
    ref = pd.DataFrame()
    ref["original"] = x_original
    ref["bucket"] = x_bucketed
    ref["y"] = y

    table = ref.groupby("bucket")["original"].agg(["min", "max", "count"]).reset_index()
    table["range"] = table["min"].astype(str) + " - " + table["max"].astype(str)
    table = table.drop(columns=["min", "max"])
    table = table[["bucket", "range", "count"]]

    # Add bucket map as range, if available.
    if bucket_mapping:
        # note that the buckets are sorted, so we can use that order
        table["range"] = table["bucket"].replace(table["bucket"].unique(), bucket_mapping.get_map())

    # Add counts %
    table["count %"] = round((table["count"] / table["count"].sum()) * 100, 2)
    # table["count %"] = table["count %"].astype(str) + "%"

    # Add event rates
    er = ref.groupby(["bucket", "y"]).agg({"y": ["count"]}).reset_index()
    er.columns = [" ".join(col).strip() for col in er.columns.values]
    er = er.pivot(index="bucket", columns="y", values="y count")
    er = er.rename(columns={0: "Non-event", 1: "Event"})
    er["Event Rate"] = round((er["Event"] / (er["Event"] + er["Non-event"])) * 100, 2)
    # er["Event Rate"] = er["Event Rate"].astype(str) + "%"
    table = table.merge(er, how="left", on="bucket")

    # Add WoE and IV
    table["% Event"] = table["Event"] / table["Event"].sum()
    table["% Non Event"] = table["Non-event"] / table["Non-event"].sum()
    table["WoE"] = ((table["% Event"] + epsilon) / (table["% Non Event"] + epsilon)).apply(lambda x: np.log(x))
    table["WoE"] = round(table["WoE"], 3)
    table["IV"] = (table["% Event"] - table["% Non Event"]) * table["WoE"]
    table["IV"] = round(table["IV"], 3)
    table = table.drop(columns=["% Event", "% Non Event"])

    return table


def create_report(
    X: pd.DataFrame,
    y: np.array,
    column: str,
    bucketer,
    # bucketmapping: Optional[BucketMapping] = None,
    epsilon=0.00001,
    verbose=False,
) -> pd.DataFrame:
    """Calculates summary statistics for a bucket generated by a skorecard bucketing object.

    This report currently works for just 1 column at a time.

    ``python
    from skorecard import datasets
    from skorecard.bucketers import DecisionTreeBucketer
    X, y = datasets.load_uci_credit_card(return_X_y=True)

    # make sure that those cases
    specials = {
        "LIMIT_BAL":{
            "=50000":[50000],
            "in [20001,30000]":[20000,30000],
            }
    }

    dt_bucketer = DecisionTreeBucketer(variables=['LIMIT_BAL'], specials = specials)
    dt_bucketer.fit(X, y)
    dt_bucketer.transform(X)

    df_report = create_report(X,y,column="LIMIT_BAL", bucketer= dt_bucketer)

    ```

    Args:
         X (pd.DataFrame): features
         y (np.array): target
         column (str): column for which you want the report
         bucketer: Skorecard bucket object
         epsilon(float):
         verbose(boolean)

    Returns:
        df (pandas DataFrame): reporting df
    """
    X = X.copy()

    if isinstance(bucketer, UserInputBucketer):
        bucket_dict = bucketer.features_bucket_mapping_.maps
    else:
        bucket_dict = bucketer.features_bucket_mapping_

    bucket_mapping = bucket_dict[column]
    X_transform = bucketer.transform(X)[[column]]
    X_transform = X_transform.rename(columns={column: "Bucket_id"})

    X_transform["Event"] = y

    stats = X_transform.groupby("Bucket_id", as_index=False).agg(
        def_rate=pd.NamedAgg(column="Event", aggfunc="mean"),
        Event=pd.NamedAgg(column="Event", aggfunc="sum"),
        Count=pd.NamedAgg(column="Bucket_id", aggfunc="count"),
    )

    stats["bin_labels"] = stats["Bucket_id"].map(bucket_mapping.labels)
    stats["Count (%)"] = (stats["Count"] / stats["Count"].sum()).apply(lambda x: np.round(x * 100, 2))

    stats["Non Event"] = stats["Count"] - stats["Event"]
    # Default rates
    stats["Event Rate"] = stats["Event"] / stats["Count"]  # todo: can we divide by 0 accidentally?

    stats["% Event"] = stats["Event"] / stats["Event"].sum()
    stats["% Non Event"] = stats["Non Event"] / stats["Non Event"].sum()

    stats["WoE"] = ((stats["% Event"] + epsilon) / (stats["% Non Event"] + epsilon)).apply(lambda x: np.log(x))

    stats["IV"] = (stats["% Event"] - stats["% Non Event"]) * stats["WoE"]

    if verbose:
        iv_total = stats["IV"].sum()
        print(f"IV for {column} = {np.round(iv_total, 4)}")
    columns = [
        "Bucket_id",
        "bin_labels",
        "Count",
        "Count (%)",
        "Event",
        "% Event",
        "Non Event",
        "% Non Event",
        "Event Rate",
        "WoE",
        "IV",
    ]
    return stats.sort_values(by="Bucket_id")[columns]
