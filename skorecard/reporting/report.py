import pandas as pd
import numpy as np
from typing import Union, Optional

from skorecard.bucket_mapping import BucketMapping

# TODO: missing should be a separate row


def bucket_table(
    x_original: pd.Series,
    x_bucketed: pd.Series,
    y: Union[pd.Series, np.array],
    bucket_mapping: Optional[BucketMapping] = None,
    epsilon: float = 0.00001,
) -> pd.DataFrame:
    """Create a table with results of bucketing.

    Example:

    ```python
    from skorecard import datasets
    from skorecard.bucketers import DecisionTreeBucketer
    from skorecard.reporting import bucket_table

    X, y = datasets.load_uci_credit_card(return_X_y=True)
    db = DecisionTreeBucketer(max_n_bins=7, min_bin_size=0.05)
    X_trans = db.fit_transform(X, y)
    x_original = X['BILL_AMT1']
    x_bucketed = X_trans['BILL_AMT1']

    bucket_table(x_original, x_bucketed, y)

    # or with a bucket_mapping for nice bucket range printing
    bucket_table(x_original, x_bucketed, y,
        bucket_mapping = db.features_bucket_mapping_.get('BILL_AMT1')
    )
    ```
    """
    ref = pd.DataFrame()
    ref["original"] = x_original
    ref["bucket"] = x_bucketed
    ref["y"] = y

    table = ref.groupby("bucket")["original"].agg(["min", "max", "count"]).reset_index()
    table["range"] = table["min"].astype(str) + " - " + table["max"].astype(str)
    table = table.drop(columns=["min", "max"])
    table = table[["bucket", "range", "count"]]

    # Add bucket map as range, if available.
    if bucket_mapping:
        # note that the buckets are sorted, so we can use that order
        table["range"] = table["bucket"].replace(table["bucket"].unique(), bucket_mapping.get_map())

    # Add counts %
    table["count %"] = round((table["count"] / table["count"].sum()) * 100, 2)
    # table["count %"] = table["count %"].astype(str) + "%"

    # Add event rates
    er = ref.groupby(["bucket", "y"]).agg({"y": ["count"]}).reset_index()
    er.columns = [" ".join(col).strip() for col in er.columns.values]
    er = er.pivot(index="bucket", columns="y", values="y count")
    er = er.rename(columns={0: "Non-event", 1: "Event"})
    er["Event Rate"] = round((er["Event"] / (er["Event"] + er["Non-event"])) * 100, 2)
    # er["Event Rate"] = er["Event Rate"].astype(str) + "%"
    table = table.merge(er, how="left", on="bucket")

    # Add WoE and IV
    table["% Event"] = table["Event"] / table["Event"].sum()
    table["% Non Event"] = table["Non-event"] / table["Non-event"].sum()
    table["WoE"] = ((table["% Event"] + epsilon) / (table["% Non Event"] + epsilon)).apply(lambda x: np.log(x))
    table["WoE"] = round(table["WoE"], 3)
    table["IV"] = (table["% Event"] - table["% Non Event"]) * table["WoE"]
    table["IV"] = round(table["IV"], 3)
    table = table.drop(columns=["% Event", "% Non Event"])

    return table


def create_report(
    X: pd.DataFrame,
    y: np.array,
    column: str,
    bucketer,
    # bucketmapping: Optional[BucketMapping] = None,
    epsilon=0.00001,
    verbose=False,
) -> pd.DataFrame:
    """Calculates summary statistics for a bucket generated by a skorecard bucketing object.

    This report currently works for just 1 column at a time.

    Args:
         X (pd.DataFrame): features
         y (np.array): target
         column (str): column for which you want the report
         bucketer: Skorecard bucket object

    Returns:
        df (pandas DataFrame): reporting df
    """
    X = X.copy()
    X_transform = bucketer.transform(X)
    bucket_mapping = bucketer.features_bucket_mapping_[column]

    if bucket_mapping.type != "numerical":
        raise NotImplementedError("Currently supporting only numerical buckets")

    thresholds = np.hstack([-np.inf, bucket_mapping.map, np.inf])
    thresh_mins = thresholds[:-1]
    thresh_max = thresholds[1:]

    bins = np.sort(X_transform[column].unique())

    df = pd.DataFrame(
        {
            "Bin id": bins,
            "Min bin": thresh_mins,
            "Max bin": thresh_max,
            "Count": X_transform[column].value_counts().loc[bins].values,
            "Count (%)": X_transform[column].value_counts(normalize=True).loc[bins].values,
        }
    )

    X_transform["target"] = y

    # Default statistics
    tmp = (
        X_transform.groupby([column])["target"]
        .sum()
        .reset_index()
        .rename(columns={column: "Bin id", "target": "Event"})
    )

    # Merge defaults
    df = df.merge(tmp, how="left", on="Bin id")

    df["Non Event"] = df["Count"] - df["Event"]
    # Default rates
    df["Event Rate"] = df["Event"] / df["Count"]  # todo: can we divide by 0 accidentally?

    df["% Event"] = df["Event"] / df["Event"].sum()
    df["% Non Event"] = df["Non Event"] / df["Non Event"].sum()

    df["WoE"] = ((df["% Event"] + epsilon) / (df["% Non Event"] + epsilon)).apply(lambda x: np.log(x))

    df["IV"] = (df["% Event"] - df["% Non Event"]) * df["WoE"]

    if verbose:
        iv_total = df["IV"].sum()
        print(f"IV for {column} = {np.round(iv_total, 4)}")

    return df.sort_values(by="Bin id")
